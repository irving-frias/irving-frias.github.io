
        # robots.txt file generated by Webpack build

        # Allow all crawlers to access everything
        User-agent: *
        Disallow:

        # Noindex the /private/ folder (Example: You can specify URLs you want to be excluded from indexing)
        User-agent: *
        Disallow: /private/

        # X-Robots-Tag for controlling indexing and crawling behavior for specific file types
        # Example: Blocking indexing for PDF files
        X-Robots-Tag: noindex, nofollow
        X-Robots-Tag: noindex, nofollow, noarchive

        # End of robots.txt
    